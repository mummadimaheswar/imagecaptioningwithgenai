{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Captioning with Generative AI\n",
        "### Using BLIP Model with Conceptual Captions Approach\n",
        "\n",
        "This notebook implements an advanced image captioning system using the BLIP (Bootstrapping Language-Image Pre-training) model, which incorporates training methodologies similar to Conceptual Captions dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch transformers gradio Pillow requests numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Pre-trained BLIP Model\n",
        "\n",
        "We're using the Salesforce BLIP model which incorporates Conceptual Captions training methodology for superior image understanding and caption generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model():\n",
        "    \"\"\"\n",
        "    Load the BLIP model and processor for image captioning.\n",
        "    Returns:\n",
        "        tuple: (processor, model) - Loaded BLIP processor and model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Loading BLIP model and processor...\")\n",
        "        \n",
        "        # Load processor and model\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        \n",
        "        # Set model to evaluation mode for inference\n",
        "        model.eval()\n",
        "        \n",
        "        # Move to GPU if available\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "        \n",
        "        print(f\"Model loaded successfully on {device}!\")\n",
        "        return processor, model, device\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {str(e)}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Load the model\n",
        "processor, model, device = load_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Image Preprocessing and Caption Generation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_image(image_input):\n",
        "    \"\"\"\n",
        "    Preprocess image for the BLIP model.\n",
        "    \n",
        "    Args:\n",
        "        image_input: PIL Image, numpy array, or file path\n",
        "    \n",
        "    Returns:\n",
        "        PIL.Image: Preprocessed image\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if isinstance(image_input, str):\n",
        "            # If it's a URL\n",
        "            if image_input.startswith(('http://', 'https://')):\n",
        "                response = requests.get(image_input)\n",
        "                image = Image.open(BytesIO(response.content))\n",
        "            else:\n",
        "                # If it's a local file path\n",
        "                image = Image.open(image_input)\n",
        "        elif isinstance(image_input, np.ndarray):\n",
        "            # If it's a numpy array (from Gradio)\n",
        "            image = Image.fromarray(image_input)\n",
        "        else:\n",
        "            # If it's already a PIL Image\n",
        "            image = image_input\n",
        "        \n",
        "        # Convert to RGB if necessary\n",
        "        if image.mode != 'RGB':\n",
        "            image = image.convert('RGB')\n",
        "        \n",
        "        return image\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing image: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def generate_caption(image_input, max_length=50, num_beams=5):\n",
        "    \"\"\"\n",
        "    Generate caption for the given image using BLIP model.\n",
        "    \n",
        "    Args:\n",
        "        image_input: Input image (PIL Image, numpy array, or file path)\n",
        "        max_length (int): Maximum length of generated caption\n",
        "        num_beams (int): Number of beams for beam search\n",
        "    \n",
        "    Returns:\n",
        "        str: Generated caption\n",
        "    \"\"\"\n",
        "    if model is None or processor is None:\n",
        "        return \"Error: Model not loaded properly\"\n",
        "    \n",
        "    try:\n",
        "        # Preprocess the image\n",
        "        image = preprocess_image(image_input)\n",
        "        if image is None:\n",
        "            return \"Error: Could not process the image\"\n",
        "        \n",
        "        # Process image and generate caption\n",
        "        inputs = processor(image, return_tensors=\"pt\").to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                num_beams=num_beams,\n",
        "                early_stopping=True,\n",
        "                do_sample=False\n",
        "            )\n",
        "        \n",
        "        # Decode the generated caption\n",
        "        caption = processor.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        \n",
        "        return caption\n",
        "    \n",
        "    except Exception as e:\n",
        "        return f\"Error generating caption: {str(e)}\"\n",
        "\n",
        "print(\"Caption generation functions defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Test Caption Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a sample image URL\n",
        "test_image_url = \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=500\"\n",
        "test_caption = generate_caption(test_image_url)\n",
        "print(f\"Test Caption: {test_caption}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Advanced Gradio Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"\n",
        "    Create and return a Gradio interface for image captioning.\n",
        "    \n",
        "    Returns:\n",
        "        gr.Interface: Configured Gradio interface\n",
        "    \"\"\"\n",
        "    \n",
        "    def caption_with_options(image, max_length, num_beams):\n",
        "        \"\"\"\n",
        "        Wrapper function for Gradio interface with configurable options.\n",
        "        \"\"\"\n",
        "        if image is None:\n",
        "            return \"Please upload an image first.\"\n",
        "        \n",
        "        caption = generate_caption(image, max_length=int(max_length), num_beams=int(num_beams))\n",
        "        return caption\n",
        "    \n",
        "    # Create Gradio interface\n",
        "    interface = gr.Interface(\n",
        "        fn=caption_with_options,\n",
        "        inputs=[\n",
        "            gr.Image(type=\"numpy\", label=\"Upload Image\"),\n",
        "            gr.Slider(minimum=20, maximum=100, value=50, step=5, label=\"Max Caption Length\"),\n",
        "            gr.Slider(minimum=1, maximum=10, value=5, step=1, label=\"Number of Beams\")\n",
        "        ],\n",
        "        outputs=gr.Textbox(label=\"Generated Caption\", lines=3),\n",
        "        title=\"üñºÔ∏è AI Image Captioning with BLIP\",\n",
        "        description=\"Upload an image and get an AI-generated caption using the BLIP model trained on Conceptual Captions methodology.\",\n",
        "        article=\"### How it works: Upload your image, adjust parameters if needed, and get your AI-generated caption instantly!\",\n",
        "        theme=gr.themes.Soft(),\n",
        "        allow_flagging=\"never\"\n",
        "    )\n",
        "    \n",
        "    return interface\n",
        "\n",
        "# Create the interface\n",
        "demo = create_gradio_interface()\n",
        "print(\"Gradio interface created successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Launch the Application"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch the Gradio interface\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Launching Image Captioning Application...\")\n",
        "    demo.launch(\n",
        "        share=True,\n",
        "        server_name=\"0.0.0.0\",\n",
        "        server_port=7860,\n",
        "        show_error=True\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
