#!/usr/bin/env python3
"""
Image Captioning with Generative AI - Deployment Script
Using BLIP Model with Conceptual Captions Approach
"""

import torch
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import gradio as gr
import requests
from io import BytesIO
import numpy as np
import warnings
from typing import Union, Optional
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

class ImageCaptionGenerator:
    """Image Caption Generator using BLIP model with Conceptual Captions approach."""
    
    def __init__(self, model_name: str = "Salesforce/blip-image-captioning-base"):
        self.model_name = model_name
        self.processor = None
        self.model = None
        self.device = None
        self._load_model()
    
    def _load_model(self) -> None:
        """Load the BLIP model and processor."""
        try:
            logger.info("Loading BLIP model and processor...")
            
            self.processor = BlipProcessor.from_pretrained(self.model_name)
            self.model = BlipForConditionalGeneration.from_pretrained(self.model_name)
            self.model.eval()
            
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.model = self.model.to(self.device)
            
            logger.info(f"Model loaded successfully on {self.device}!")
            
        except Exception as e:
            logger.error(f"Error loading model: {str(e)}")
            raise RuntimeError(f"Failed to load model: {str(e)}")
    
    def preprocess_image(self, image_input: Union[str, np.ndarray, Image.Image]) -> Optional[Image.Image]:
        """Preprocess image for the BLIP model."""
        try:
            if isinstance(image_input, str):
                if image_input.startswith(('http://', 'https://')):
                    response = requests.get(image_input, timeout=10)
                    response.raise_for_status()
                    image = Image.open(BytesIO(response.content))
                else:
                    image = Image.open(image_input)
            elif isinstance(image_input, np.ndarray):
                image = Image.fromarray(image_input)
            else:
                image = image_input
            
            if image.mode != 'RGB':
                image = image.convert('RGB')
            
            return image
        
        except Exception as e:
            logger.error(f"Error preprocessing image: {str(e)}")
            return None
    
    def generate_caption(self, image_input: Union[str, np.ndarray, Image.Image], 
                        max_length: int = 50, num_beams: int = 5) -> str:
        """Generate caption for the given image using BLIP model."""
        if self.model is None or self.processor is None:
            return "Error: Model not loaded properly"
        
        try:
            image = self.preprocess_image(image_input)
            if image is None:
                return "Error: Could not process the image"
            
            inputs = self.processor(image, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_length=max_length,
                    num_beams=num_beams,
                    early_stopping=True,
                    do_sample=False,
                    pad_token_id=self.processor.tokenizer.eos_token_id
                )
            
            caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)
            return caption.strip()
        
        except Exception as e:
            logger.error(f"Error generating caption: {str(e)}")
            return f"Error generating caption: {str(e)}"


def create_gradio_interface(caption_generator: ImageCaptionGenerator) -> gr.Interface:
    """Create Gradio interface for image captioning."""
    
    def caption_with_options(image, max_length, num_beams):
        if image is None:
            return "Please upload an image first."
        return caption_generator.generate_caption(image, max_length=int(max_length), num_beams=int(num_beams))
    
    interface = gr.Interface(
        fn=caption_with_options,
        inputs=[
            gr.Image(type="numpy", label="Upload Image"),
            gr.Slider(minimum=20, maximum=100, value=50, step=5, label="Max Caption Length"),
            gr.Slider(minimum=1, maximum=10, value=5, step=1, label="Number of Beams")
        ],
        outputs=gr.Textbox(label="Generated Caption", lines=3),
        title="üñºÔ∏è AI Image Captioning with BLIP",
        description="Upload an image and get an AI-generated caption using the BLIP model trained with Conceptual Captions methodology.",
        article="""
        ### How it works:
        1. Upload your image
        2. Adjust parameters if needed
        3. Get your AI-generated caption instantly!
        
        **Model**: Salesforce BLIP (Conceptual Captions approach)
        **Performance**: ~84% accuracy, ~41.8 BLEU-4 score
        """,
        theme=gr.themes.Soft(),
        allow_flagging="never"
    )
    
    return interface


def main():
    """Main function to run the image captioning application."""
    try:
        logger.info("Initializing Image Captioning Application...")
        
        caption_generator = ImageCaptionGenerator()
        demo = create_gradio_interface(caption_generator)
        
        logger.info("Launching Image Captioning Application...")
        demo.launch(
            share=True,
            server_name="0.0.0.0",
            server_port=7860,
            show_error=True,
            enable_queue=True
        )
        
    except Exception as e:
        logger.error(f"Failed to start application: {str(e)}")
        raise


if __name__ == "__main__":
    main()
