{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Captioning Chatbot with Generative AI\n",
        "### Using BLIP Model + OpenAI GPT for Conversational Image Analysis\n",
        "\n",
        "This notebook implements an advanced conversational image captioning system that combines:\n",
        "- **BLIP** for image understanding and caption generation\n",
        "- **OpenAI GPT** for natural language conversations about images\n",
        "- **Joblib** for efficient caching and model management\n",
        "- **API Key Integration** for secure access to language models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Install Required Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install torch transformers gradio Pillow requests numpy joblib openai python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Libraries and Setup Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "import warnings\n",
        "import joblib\n",
        "import os\n",
        "import json\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "\n",
        "# OpenAI integration\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Joblib version: {joblib.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. API Key Configuration and Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class APIKeyManager:\n",
        "    \"\"\"\n",
        "    Manages API keys securely for the chatbot functionality.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.openai_client = None\n",
        "        self.api_key_file = \"config/api_keys.json\"\n",
        "        self.setup_config_directory()\n",
        "        \n",
        "    def setup_config_directory(self):\n",
        "        \"\"\"Create config directory if it doesn't exist.\"\"\"\n",
        "        os.makedirs(\"config\", exist_ok=True)\n",
        "        \n",
        "    def set_openai_key(self, api_key: str, save_to_file: bool = False) -> bool:\n",
        "        \"\"\"\n",
        "        Set OpenAI API key and initialize client.\n",
        "        \n",
        "        Args:\n",
        "            api_key (str): OpenAI API key\n",
        "            save_to_file (bool): Whether to save key to config file\n",
        "            \n",
        "        Returns:\n",
        "            bool: True if successful, False otherwise\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Test the API key\n",
        "            test_client = openai.OpenAI(api_key=api_key)\n",
        "            \n",
        "            # Test with a simple request\n",
        "            test_response = test_client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
        "                max_tokens=5\n",
        "            )\n",
        "            \n",
        "            # If successful, store the client\n",
        "            self.openai_client = test_client\n",
        "            \n",
        "            if save_to_file:\n",
        "                self._save_key_to_file(\"openai\", api_key)\n",
        "            \n",
        "            print(\"‚úÖ OpenAI API key validated and set successfully!\")\n",
        "            return True\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error setting OpenAI API key: {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    def load_keys_from_env(self) -> bool:\n",
        "        \"\"\"\n",
        "        Load API keys from environment variables.\n",
        "        \n",
        "        Returns:\n",
        "            bool: True if any keys were loaded successfully\n",
        "        \"\"\"\n",
        "        success = False\n",
        "        \n",
        "        # Try to load OpenAI key from environment\n",
        "        openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "        if openai_key:\n",
        "            if self.set_openai_key(openai_key):\n",
        "                print(\"üîë OpenAI API key loaded from environment\")\n",
        "                success = True\n",
        "        \n",
        "        return success\n",
        "    \n",
        "    def load_keys_from_file(self) -> bool:\n",
        "        \"\"\"\n",
        "        Load API keys from config file.\n",
        "        \n",
        "        Returns:\n",
        "            bool: True if any keys were loaded successfully\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(self.api_key_file):\n",
        "                return False\n",
        "            \n",
        "            with open(self.api_key_file, 'r') as f:\n",
        "                keys = json.load(f)\n",
        "            \n",
        "            success = False\n",
        "            if 'openai' in keys:\n",
        "                if self.set_openai_key(keys['openai']):\n",
        "                    print(\"üîë OpenAI API key loaded from config file\")\n",
        "                    success = True\n",
        "            \n",
        "            return success\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error loading keys from file: {str(e)}\")\n",
        "            return False\n",
        "    \n",
        "    def _save_key_to_file(self, service: str, api_key: str):\n",
        "        \"\"\"\n",
        "        Save API key to config file.\n",
        "        \n",
        "        Args:\n",
        "            service (str): Service name (e.g., 'openai')\n",
        "            api_key (str): API key to save\n",
        "        \"\"\"\n",
        "        try:\n",
        "            keys = {}\n",
        "            if os.path.exists(self.api_key_file):\n",
        "                with open(self.api_key_file, 'r') as f:\n",
        "                    keys = json.load(f)\n",
        "            \n",
        "            keys[service] = api_key\n",
        "            \n",
        "            with open(self.api_key_file, 'w') as f:\n",
        "                json.dump(keys, f, indent=2)\n",
        "            \n",
        "            print(f\"üîë {service} API key saved to config file\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error saving key to file: {str(e)}\")\n",
        "    \n",
        "    def is_openai_available(self) -> bool:\n",
        "        \"\"\"Check if OpenAI client is available.\"\"\"\n",
        "        return self.openai_client is not None\n",
        "    \n",
        "    def get_openai_client(self):\n",
        "        \"\"\"Get OpenAI client instance.\"\"\"\n",
        "        return self.openai_client\n",
        "\n",
        "# Initialize API key manager\n",
        "api_manager = APIKeyManager()\n",
        "\n",
        "# Try to load keys from environment or config file\n",
        "if not api_manager.load_keys_from_env():\n",
        "    api_manager.load_keys_from_file()\n",
        "\n",
        "print(f\"OpenAI API available: {api_manager.is_openai_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Enhanced Model Management with Joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directories for caching\n",
        "MODEL_CACHE_DIR = Path(\"model_cache\")\n",
        "RESULTS_CACHE_DIR = Path(\"results_cache\")\n",
        "CHAT_CACHE_DIR = Path(\"chat_cache\")\n",
        "\n",
        "for directory in [MODEL_CACHE_DIR, RESULTS_CACHE_DIR, CHAT_CACHE_DIR]:\n",
        "    directory.mkdir(exist_ok=True)\n",
        "\n",
        "def save_model_components(processor, model, device, filepath=\"model_cache/blip_components.pkl\"):\n",
        "    \"\"\"\n",
        "    Save model components using joblib for efficient serialization.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model_cpu = model.cpu()\n",
        "        \n",
        "        model_components = {\n",
        "            'processor': processor,\n",
        "            'model_state_dict': model_cpu.state_dict(),\n",
        "            'model_config': model_cpu.config,\n",
        "            'device_info': str(device),\n",
        "            'saved_at': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        joblib.dump(model_components, filepath, compress=3)\n",
        "        print(f\"Model components saved to {filepath}\")\n",
        "        \n",
        "        model.to(device)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model components: {str(e)}\")\n",
        "\n",
        "def load_model_components(filepath=\"model_cache/blip_components.pkl\"):\n",
        "    \"\"\"\n",
        "    Load model components using joblib.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(filepath):\n",
        "            print(f\"Model cache not found. Loading from HuggingFace...\")\n",
        "            return load_model_from_hub()\n",
        "        \n",
        "        print(f\"Loading model components from cache...\")\n",
        "        model_components = joblib.load(filepath)\n",
        "        \n",
        "        processor = model_components['processor']\n",
        "        model_state_dict = model_components['model_state_dict']\n",
        "        model_config = model_components['model_config']\n",
        "        \n",
        "        model = BlipForConditionalGeneration(model_config)\n",
        "        model.load_state_dict(model_state_dict)\n",
        "        model.eval()\n",
        "        \n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "        \n",
        "        print(f\"‚úÖ Model loaded from cache on {device}!\")\n",
        "        return processor, model, device\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading from cache: {str(e)}. Loading from HuggingFace...\")\n",
        "        return load_model_from_hub()\n",
        "\n",
        "def load_model_from_hub():\n",
        "    \"\"\"\n",
        "    Load the BLIP model and processor from HuggingFace Hub.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Loading BLIP model from HuggingFace...\")\n",
        "        \n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        \n",
        "        model.eval()\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        model = model.to(device)\n",
        "        \n",
        "        print(f\"‚úÖ Model loaded from HuggingFace on {device}!\")\n",
        "        \n",
        "        # Save to cache for future use\n",
        "        save_model_components(processor, model, device)\n",
        "        \n",
        "        return processor, model, device\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading model: {str(e)}\")\n",
        "        return None, None, None\n",
        "\n",
        "# Load the model\n",
        "processor, model, device = load_model_components()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Chatbot Integration and Conversation Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImageCaptionChatbot:\n",
        "    \"\"\"\n",
        "    Conversational AI chatbot that combines image captioning with natural language chat.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, api_manager, processor, model, device):\n",
        "        self.api_manager = api_manager\n",
        "        self.processor = processor\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.conversation_history = []\n",
        "        self.current_image_context = None\n",
        "        self.current_image_caption = None\n",
        "        \n",
        "        # System prompt for the chatbot\n",
        "        self.system_prompt = \"\"\"\n",
        "You are an AI assistant specialized in image analysis and conversation. \n",
        "You can:\n",
        "1. Generate detailed captions for images using computer vision\n",
        "2. Answer questions about images you've analyzed\n",
        "3. Engage in natural conversation about visual content\n",
        "4. Provide insights about composition, objects, scenes, and context\n",
        "\n",
        "When discussing images, be descriptive, engaging, and insightful.\n",
        "If no image is currently loaded, let the user know they should upload one first.\n",
        "\"\"\"\n",
        "    \n",
        "    def generate_caption(self, image_input, max_length=50, num_beams=5, use_cache=True):\n",
        "        \"\"\"\n",
        "        Generate caption for image with caching.\n",
        "        \"\"\"\n",
        "        if self.model is None or self.processor is None:\n",
        "            return \"Error: Model not loaded properly\"\n",
        "        \n",
        "        try:\n",
        "            image = self._preprocess_image(image_input)\n",
        "            if image is None:\n",
        "                return \"Error: Could not process the image\"\n",
        "            \n",
        "            # Check cache if enabled\n",
        "            params = {'max_length': max_length, 'num_beams': num_beams}\n",
        "            if use_cache:\n",
        "                image_hash = self._get_image_hash(image)\n",
        "                if image_hash:\n",
        "                    cached_caption = self._load_caption_cache(image_hash, params)\n",
        "                    if cached_caption:\n",
        "                        self.current_image_caption = cached_caption\n",
        "                        self.current_image_context = image\n",
        "                        return f\"{cached_caption} [Cached]\"\n",
        "            \n",
        "            # Generate new caption\n",
        "            inputs = self.processor(image, return_tensors=\"pt\").to(self.device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                generated_ids = self.model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=max_length,\n",
        "                    num_beams=num_beams,\n",
        "                    early_stopping=True,\n",
        "                    do_sample=False\n",
        "                )\n",
        "            \n",
        "            caption = self.processor.decode(generated_ids[0], skip_special_tokens=True)\n",
        "            \n",
        "            # Update context\n",
        "            self.current_image_caption = caption\n",
        "            self.current_image_context = image\n",
        "            \n",
        "            # Save to cache\n",
        "            if use_cache and image_hash:\n",
        "                self._save_caption_cache(image_hash, caption, params)\n",
        "            \n",
        "            return caption\n",
        "            \n",
        "        except Exception as e:\n",
        "            return f\"Error generating caption: {str(e)}\"\n",
        "    \n",
        "    def chat_about_image(self, user_message: str, model=\"gpt-3.5-turbo\") -> str:\n",
        "        \"\"\"\n",
        "        Have a conversation about the current image using OpenAI GPT.\n",
        "        \"\"\"\n",
        "        if not self.api_manager.is_openai_available():\n",
        "            return \"‚ùå OpenAI API not available. Please set your API key first.\"\n",
        "        \n",
        "        try:\n",
        "            # Prepare context\n",
        "            context_message = \"\"\n",
        "            if self.current_image_caption:\n",
        "                context_message = f\"Current image caption: '{self.current_image_caption}'\"\n",
        "            else:\n",
        "                context_message = \"No image currently loaded.\"\n",
        "            \n",
        "            # Prepare messages\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "                {\"role\": \"system\", \"content\": context_message}\n",
        "            ]\n",
        "            \n",
        "            # Add conversation history (last 10 exchanges)\n",
        "            messages.extend(self.conversation_history[-20:])\n",
        "            \n",
        "            # Add current user message\n",
        "            messages.append({\"role\": \"user\", \"content\": user_message})\n",
        "            \n",
        "            # Get response from OpenAI\n",
        "            client = self.api_manager.get_openai_client()\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=messages,\n",
        "                max_tokens=500,\n",
        "                temperature=0.7\n",
        "            )\n",
        "            \n",
        "            assistant_message = response.choices[0].message.content\n",
        "            \n",
        "            # Update conversation history\n",
        "            self.conversation_history.append({\"role\": \"user\", \"content\": user_message})\n",
        "            self.conversation_history.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "            \n",
        "            return assistant_message\n",
        "            \n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error in chat: {str(e)}\"\n",
        "    \n",
        "    def clear_conversation(self):\n",
        "        \"\"\"Clear conversation history.\"\"\"\n",
        "        self.conversation_history = []\n",
        "        return \"Conversation history cleared.\"\n",
        "    \n",
        "    def get_conversation_summary(self) -> str:\n",
        "        \"\"\"Get a summary of the current conversation.\"\"\"\n",
        "        if not self.conversation_history:\n",
        "            return \"No conversation yet.\"\n",
        "        \n",
        "        return f\"Conversation has {len(self.conversation_history)//2} exchanges. Current image: {'Yes' if self.current_image_caption else 'No'}\"\n",
        "    \n",
        "    def _preprocess_image(self, image_input):\n",
        "        \"\"\"Preprocess image for the BLIP model.\"\"\"\n",
        "        try:\n",
        "            if isinstance(image_input, str):\n",
        "                if image_input.startswith(('http://', 'https://')):\n",
        "                    response = requests.get(image_input)\n",
        "                    image = Image.open(BytesIO(response.content))\n",
        "                else:\n",
        "                    image = Image.open(image_input)\n",
        "            elif isinstance(image_input, np.ndarray):\n",
        "                image = Image.fromarray(image_input)\n",
        "            else:\n",
        "                image = image_input\n",
        "            \n",
        "            if image.mode != 'RGB':\n",
        "                image = image.convert('RGB')\n",
        "            \n",
        "            return image\n",
        "        except Exception as e:\n",
        "            print(f\"Error preprocessing image: {str(e)}\")\n",
        "            return None\n",
        "    \n",
        "    def _get_image_hash(self, image):\n",
        "        \"\"\"Generate hash for image caching.\"\"\"\n",
        "        try:\n",
        "            img_byte_arr = BytesIO()\n",
        "            image.save(img_byte_arr, format='PNG')\n",
        "            img_bytes = img_byte_arr.getvalue()\n",
        "            return hashlib.md5(img_bytes).hexdigest()\n",
        "        except:\n",
        "            return None\n",
        "    \n",
        "    def _save_caption_cache(self, image_hash, caption, params):\n",
        "        \"\"\"Save caption to cache.\"\"\"\n",
        "        try:\n",
        "            cache_data = {\n",
        "                'caption': caption,\n",
        "                'params': params,\n",
        "                'timestamp': datetime.now().isoformat()\n",
        "            }\n",
        "            filepath = f\"results_cache/caption_cache_{image_hash[:16]}.pkl\"\n",
        "            joblib.dump(cache_data, filepath, compress=3)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not save to cache: {str(e)}\")\n",
        "    \n",
        "    def _load_caption_cache(self, image_hash, params):\n",
        "        \"\"\"Load caption from cache.\"\"\"\n",
        "        try:\n",
        "            filepath = f\"results_cache/caption_cache_{image_hash[:16]}.pkl\"\n",
        "            if not os.path.exists(filepath):\n",
        "                return None\n",
        "            \n",
        "            cache_data = joblib.load(filepath)\n",
        "            if cache_data['params'] == params:\n",
        "                print(\"Using cached result...\")\n",
        "                return cache_data['caption']\n",
        "            \n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not load from cache: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "# Initialize chatbot\n",
        "chatbot = ImageCaptionChatbot(api_manager, processor, model, device)\n",
        "print(\"‚úÖ Image Caption Chatbot initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test the System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test image captioning\n",
        "test_image_url = \"https://images.unsplash.com/photo-1506905925346-21bda4d32df4?w=500\"\n",
        "print(\"Testing image captioning...\")\n",
        "test_caption = chatbot.generate_caption(test_image_url)\n",
        "print(f\"Caption: {test_caption}\")\n",
        "\n",
        "# Test chatbot functionality (if API key is available)\n",
        "if api_manager.is_openai_available():\n",
        "    print(\"\\nTesting chatbot conversation...\")\n",
        "    response = chatbot.chat_about_image(\"What can you tell me about this image?\")\n",
        "    print(f\"Chatbot: {response}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è OpenAI API not available. Chatbot features will be limited.\")\n",
        "    print(\"To enable full functionality, set your OpenAI API key using the interface below.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Advanced Gradio Interface with Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_chatbot_interface():\n",
        "    \"\"\"\n",
        "    Create comprehensive Gradio interface with chatbot functionality.\n",
        "    \"\"\"\n",
        "    \n",
        "    with gr.Blocks(theme=gr.themes.Soft(), title=\"AI Image Captioning Chatbot\") as interface:\n",
        "        \n",
        "        # Header\n",
        "        gr.Markdown(\"\"\"\n",
        "        # ü§ñ AI Image Captioning Chatbot\n",
        "        ### Powered by BLIP + OpenAI GPT + Joblib Caching\n",
        "        \n",
        "        Upload an image and chat about it! The AI can generate captions and answer questions about your images.\n",
        "        \"\"\")\n",
        "        \n",
        "        # API Key Configuration Section\n",
        "        with gr.Accordion(\"üîë API Configuration\", open=not api_manager.is_openai_available()):\n",
        "            with gr.Row():\n",
        "                api_key_input = gr.Textbox(\n",
        "                    label=\"OpenAI API Key\", \n",
        "                    placeholder=\"sk-...\", \n",
        "                    type=\"password\",\n",
        "                    scale=3\n",
        "                )\n",
        "                set_key_btn = gr.Button(\"Set API Key\", scale=1)\n",
        "                save_key_checkbox = gr.Checkbox(label=\"Save to file\", value=False)\n",
        "            \n",
        "            api_status = gr.Textbox(\n",
        "                label=\"API Status\", \n",
        "                value=f\"OpenAI: {'‚úÖ Connected' if api_manager.is_openai_available() else '‚ùå Not Connected'}\",\n",
        "                interactive=False\n",
        "            )\n",
        "        \n",
        "        # Main Interface
        with gr.Row():
            # Left Column - Image Upload and Controls
            with gr.Column(scale=1):
                gr.Markdown("### üì∏ Image Upload")
                image_input = gr.Image(type="numpy", label="Upload Image")
                
                with gr.Row():
                    max_length = gr.Slider(20, 100, 50, step=5, label="Max Caption Length")
                    num_beams = gr.Slider(1, 10, 5, step=1, label="Number of Beams")
                
                use_cache = gr.Checkbox(value=True, label="Use Caching")
                generate_caption_btn = gr.Button("üéØ Generate Caption", variant="primary")
                
                # Caption Output
                caption_output = gr.Textbox(
                    label="Generated Caption", 
                    lines=3, 
                    interactive=False
                )
                
                # Cache Management
                with gr.Accordion("üìä Cache Management", open=False):
                    cache_info_display = gr.Textbox(
                        label="Cache Status",
                        value="Click 'Refresh Info' to see cache status",
                        interactive=False
                    )
                    with gr.Row():
                        refresh_cache_btn = gr.Button("üîÑ Refresh Info")
                        clear_cache_btn = gr.Button("üóëÔ∏è Clear Cache")
            
            # Right Column - Chatbot Interface
            with gr.Column(scale=1):
                gr.Markdown("### üí¨ Chat About Your Image")
                
                chatbot_display = gr.Chatbot(
                    label="Conversation",
                    height=400,
                    show_label=True
                )
                
                with gr.Row():
                    chat_input = gr.Textbox(
                        label="Your Message",
                        placeholder="Ask me anything about the image...",
                        scale=4
                    )
                    chat_submit_btn = gr.Button("Send", scale=1, variant="primary")
                
                with gr.Row():
                    clear_chat_btn = gr.Button("üóëÔ∏è Clear Chat")
                    chat_status = gr.Textbox(
                        label="Chat Status",
                        value="Ready to chat!",
                        interactive=False,
                        scale=2
                    )
                
                # Model Selection
                with gr.Accordion("üéõÔ∏è Chat Settings", open=False):
                    model_choice = gr.Dropdown(
                        choices=["gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"],
                        value="gpt-3.5-turbo",
                        label="GPT Model"
                    )
        
        # Batch Processing Section
        with gr.Accordion("üîÑ Batch Processing", open=False):
            gr.Markdown("### Process Multiple Images")
            with gr.Row():
                batch_files = gr.File(
                    file_count="multiple",
                    file_types=["image"],
                    label="Upload Multiple Images"
                )
                batch_process_btn = gr.Button("Process Batch", variant="secondary")
            
            batch_results = gr.DataFrame(
                headers=["Image", "Caption", "Status"],
                label="Batch Results"
            )
        
        # Event Handlers
        def set_api_key(api_key, save_to_file):
            """Set OpenAI API key."""
            if not api_key.strip():
                return "‚ùå Please enter an API key", "‚ùå Not Connected"
            
            success = api_manager.set_openai_key(api_key.strip(), save_to_file)
            if success:
                return "‚úÖ API key set successfully!", f"OpenAI: ‚úÖ Connected"
            else:
                return "‚ùå Invalid API key", f"OpenAI: ‚ùå Not Connected"
        
        def generate_caption_wrapper(image, max_len, beams, cache):
            """Generate caption wrapper for Gradio."""
            if image is None:
                return "Please upload an image first."
            
            caption = chatbot.generate_caption(
                image, 
                max_length=int(max_len), 
                num_beams=int(beams), 
                use_cache=cache
            )
            return caption
        
        def chat_with_bot(message, history, model):
            """Handle chat interaction."""
            if not message.strip():
                return history, ""
            
            if not api_manager.is_openai_available():
                response = "‚ùå OpenAI API not available. Please set your API key first."
            else:
                response = chatbot.chat_about_image(message, model)
            
            history.append([message, response])
            return history, ""
        
        def clear_conversation():
            """Clear chat conversation."""
            chatbot.clear_conversation()
            return [], "Conversation cleared!"
        
        def get_cache_info():
            """Get current cache information."""
            try:
                model_cache_exists = os.path.exists('model_cache/blip_components.pkl')
                results_cache_count = len(list(Path("results_cache").glob('*.pkl')))
                
                model_size = 0
                results_size = 0
                
                if model_cache_exists:
                    model_size = os.path.getsize('model_cache/blip_components.pkl') / (1024 * 1024)
                
                for cache_file in Path("results_cache").glob('*.pkl'):
                    results_size += os.path.getsize(cache_file)
                results_size /= (1024 * 1024)
                
                return f\"\"\"Cache Status:
Model Cache: {'‚úÖ' if model_cache_exists else '‚ùå'} ({model_size:.2f} MB)
Results Cache: {results_cache_count} files ({results_size:.2f} MB)
Chat Cache: {len(chatbot.conversation_history)//2} conversations\"\"\"\n
